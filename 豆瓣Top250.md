# 爬取豆瓣Top250

## 一、模块使用补充

### 1. urllib

Urllib 主要用来 模拟浏览器对指定的 url 发起请求，获得对应的网页源码

* 指定 请求头

  ~~~python
  url = "https://movie.douban.com/top250?start="
  headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36"
  }
  
  req = urllib.request.Request(url=url, headers=headers)
  ~~~

* 发起请求，并设定错误处理

  ~~~python
  try:
    res = urllib.request.urlopen(req)
    html = res.read().decode('utf-8')
  except urllib.error.URLError as e:
    if hasattr(e, "code"):
      print("发生了一些错误...", e.code)
    if hasattr(e, "reason"):
      print("原因：", e.reason)
  ~~~

### 2. beautiful soup

#### 节点类型

BS 将复杂的 HTML 文档转换成一个树形结构，每个节点都是 Python对象，所有的对象可以归纳为如下 4 种：

- Tag：标签及其内容，拿到它所找的第一个内容

  ~~~python
  file = open("./test.html", "rb")
  html = file.read()
  bs = BeautifulSoup(html, "html.parser")
  print(bs.title)				 # 输出 <title>百度一下，你就知道</title>
  print(type(bs.title))  # 输出 <class 'bs4.element.Tag'>
  # 使用 bs.title.attrs 可以获取该标签的属性，以键值对的形式输出
  ~~~

- NavigableString：标签里的内容（字符串）

  ~~~python
  print(bs.title.string)				# 输出 百度一下，你就知道
  print(type(bs.title.string))	# 输出 <class 'bs4.element.NavigableString'>
  ~~~

- BeautifulSoup：表示整个文档

  ~~~python
  print(bs) 				# 输出 整个 html 文档
  print(bs.name)		# 输出 [document]
  print(type(bs))		# 输出 <class 'bs4.BeautifulSoup'>
  ~~~

- Comment：是一个特殊的 NavigableString，输出的内容不包含注释符号 "<!-- -->"

  HTML 部分代码：

  ~~~html
  <a href="" class="mnav"><!--新闻--></a>
  <a href="" class="mnav">新闻</a>
  ~~~

  输出结果：

  ~~~python
  print(bs.a.string)				# 输出 新闻
  print(type(bs.a.string))	# 输出 <class 'bs4.element.Comment'>
  ~~~

#### 文档遍历

常用：

- node.contents 获取 node 的所有子节点，返回一个 list

- 其它方法详见 <a href="https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/#">BS4 官方文档</a>

#### 文档搜索（常用）

- find_all()，字符串过滤，查找与字符串完全匹配的内容

  ~~~python
  t_list = bs.find_all("a")  # 找出所有的 a 标签
  ~~~

- search()，正则表达式匹配

  ~~~python
  t_list = bs.find_all(re.compile("a"))  # 找出所有包含 "a" 的标签
  ~~~

- 根据函数要求来搜索

  ~~~python
  def name_is_exist(tag):
    return tag.has_attr("name")
  
  t_list = bs.find_all(name_is_exist)  # 找出包含 "name" 属性的标签
  ~~~

- kwargs 参数搜索

  ~~~python
  t_list = bs.find_all(id="head")  # 找出 id="head" 的标签
  t_list = bs.find_all(class_="mnav")  # 找出 class="mnav" 的标签，注意下划线
  ~~~

- text 参数

  ~~~python
  t_list = bs.find_all(text= re.compile('\d'))  # 找出所有包含数字的标签内容（字符串）
  ~~~

- limit 参数

  ~~~python
  t_list = bs.find_all("a", limit=3)	# 限制数量，找出前3个 a 标签
  ~~~

- select 选择器

  ~~~python
  t_list = bs.select('title')  # 找出 title 标签
  t_list = bs.select(".mnav")  # 找出 mnav类 的所有标签
  t_list = bs.select("a[class='bri']")	# 找出 class="bri" 的a标签
  t_list = bs.select("head > title")	# 通过子标签来查找
  t_list = bs.select(".mnav ~ .bri")	# 通过兄弟标签查找
  print(t_list[0].get_text())		# 得到文本
  ~~~



## 二、准备工作

* 导入模块

  ~~~python
  from bs4 import BeautifulSoup     # 网页解析，获取数据
  import xlwt   # 进行 excel 操作
  import re   # 正则表达式，进行文字匹配
  import sqlite3  # 进行 sqlite 数据库操作
  import urllib.request, urllib.error   # 指定 url，获取网页数据
  ~~~

* 创建入口

  ~~~python
  def main():
    print('OK')
    
  if __name__ == '__main__':
      main()  # 调用函数
  ~~~

* 解决 ssl 报错

  ~~~python
  import ssl
  ssl._create_default_https_context = ssl._create_unverified_context
  ~~~



## 三、开始编写函数

* 获取网页源码

  ~~~python
  # 得到指定 url 的 网页内容
  def askURL(url):
    # 用户代理，告诉豆瓣服务器，咱们是正儿八经的浏览器，可以接受正常的网页数据
    head = {
      "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36"
    }
  
    req = urllib.request.Request(headers=head, url=url)
    html = ""
    try:
      res = urllib.request.urlopen(req)
      html = res.read().decode('utf-8')
    except urllib.error.URLError as e:
      if hasattr(e, "code"):
        print("发生了一些错误...", e.code)
      if hasattr(e, "reason"):
        print("原因：", e.reason)
  
    return html
  ~~~

* 创建正则表达式匹配对象

  ~~~python
  # 影片详情链接
  findLink = re.compile(r'<a href="(.*?)">')   # 创建正则表达式对象，表示规则（字符串的模式）
  # 影片图片链接
  findImgSrc = re.compile(r'<img.*src="(.*?)"', re.S)
  # 影片的名字
  findTitle = re.compile(r'<span class="title">(.*)</span>')
  # 影片的评分
  findRating = re.compile(r'<span class="rating_num" property="v:average">(.*)</span>')
  # 影片评价人数
  findJudge = re.compile(r'<span>(\d*)人评价</span>')
  # 影片概况
  findInq = re.compile(r'<span class="inq">(.*)</span>')
  # 影片的相关信息
  findBd = re.compile(r'<p class="">(.*?)</p>', re.S)
  ~~~

* 根据正则获取相关数据

  ~~~python
  # 根据url 爬取网页 获取数据
  def getData(baseURL):
    dataList = []
    for i in range(0, 10):   # 调用 10 次，发出 10 次请求
      url = baseURL + str(i * 25)
      html = askURL(url)   # 保存每次获取到的网页源码
      # 2.逐一解析数据
      soup = BeautifulSoup(html, "html.parser")
      for item in soup.find_all('div', class_="item"):   # 查找符合要求的字符串，形成列表
        # print(item)   # 测试 电影 item 全部信息
        data = []   # 保存一部电影的所有信息
        item = str(item)
  
        link = re.findall(findLink, item)[0]   # re 通过 正则表达式查找指定的字符串
        data.append(link)     # 添加详情链接
  
        imgSrc = re.findall(findImgSrc, item)[0]
        data.append(imgSrc)   # 添加图片
  
        titles = re.findall(findTitle, item)  # 片名可能有中文，可能有外文
        if(len(titles) == 2):
          cTitle = titles[0]   # 添加中文名
          data.append(cTitle)
          oTitle = titles[1].replace("/", "")
          oTitle = oTitle.replace("\xa0", "")
          data.append(oTitle)  # 添加外文名
        else:
          data.append(titles[0])
          data.append(" ")  # 没有外文名留空
  
        rating = re.findall(findRating, item)[0]
        data.append(rating)   # 添加评分
  
        judgeNum = re.findall(findJudge, item)[0]
        data.append(judgeNum)  # 添加评价人数
  
        inq = re.findall(findInq, item)
        if len(inq) != 0:
          inq = inq[0].replace("。", "") # 去掉句号
          data.append(inq)   # 添加一句话概述
        else:
          data.append(" ")    # 没有概述，留空
  
        bd = re.findall(findBd, item)[0]
        bd = re.sub('<br(\s+)?/>(\s+)?', " ", bd)  # 去掉 br
        bd = re.sub('/', " ", bd)   # 替换 /
        bd = bd.replace("\xa0", "")
        data.append(bd.strip())   # 去掉前后的空格
  
        dataList.append(data)   # 把处理好的一部电影信息放入 dataList
  
    return dataList
  ~~~

* 保存数据到 Excelss

  ~~~python
  # 保存数据
  def saveData(dataList, savePath):
    print("Saving data......")
    workBook = xlwt.Workbook(encoding='utf-8')  # 创建 workBook 对象
    workSheet = workBook.add_sheet('豆瓣Top250', cell_overwrite_ok=True)  # 创建工作表
    col = ('详情链接', '电影封面','中文名','外文名','评分','评价人数','概况','相关信息')
    for i in range(0, 8):
      workSheet.write(0, i, col[i])   # 写入列名
  
    for i in range(0, 250):
      print("正在写入第%d条。。。" % (i+1))
      data = dataList[i]
      for j in range(0, 8):
        workSheet.write(i+1, j, data[j])  # 数据写入
  
    workBook.save(savePath)    # 保存
    print("保存完毕！")
  ~~~

  

